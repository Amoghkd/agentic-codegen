# LLM Configuration for MSC Framework
# This file defines LLM providers and agent-specific configurations

# Available LLM Providers
providers:
  google:
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    models:
      - "gemini-1.5-flash-latest"
      - "gemini-1.5-pro-latest"
      - "gemini-pro"
    default_params:
      temperature: 0.1
      
  openai:
    class: "langchain_openai.ChatOpenAI"
    models:
      - "gpt-4o"
      - "gpt-4o-mini"
      - "gpt-3.5-turbo"
    default_params:
      temperature: 0.1
      
  anthropic:
    class: "langchain_anthropic.ChatAnthropic"
    models:
      - "claude-3-5-sonnet-20241022"
      - "claude-3-haiku-20240307"
    default_params:
      temperature: 0.1

  ollama:
    class: "langchain_google_genai.ChatGoogleGenerativeAI"  # Fallback to Gemini instead of Ollama
    models:
      - "gemini-1.5-flash-latest"
    default_params:
      temperature: 0.1

# Agent-specific LLM configurations
agents:
  planner:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "Strategic planning and architecture design"
    
  agentic_planner:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "Autonomous multi-agent planning with Graph-of-Thoughts"
    
  code_generator:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.2
    description: "Code generation from designs and requirements"
    
  corrector:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "Code correction and patching"
    
  critique:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "Code review and quality assessment"
    
  reasoner:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "Symbolic reasoning and pseudocode refinement"
    
  verifier:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.0
    description: "Code verification and testing (if LLM needed)"

  docker_agent:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.3
    description: "Docker image analysis and specification generation"

  cmd_modifier:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.1
    description: "AI-powered Docker CMD modification and optimization"

  dynamic_problem_solver:
    provider: "google"
    model: "gemini-1.5-flash-latest"
    temperature: 0.2
    description: "Dynamic analysis and solution generation for any error type"

# Global defaults
defaults:
  provider: "google"
  model: "gemini-1.5-flash-latest"
  temperature: 0.1
  max_tokens: 4096
  timeout: 30

# Environment variables mapping
env_vars:
  google: "GOOGLE_API_KEY"
  openai: "OPENAI_API_KEY"
  anthropic: "ANTHROPIC_API_KEY"
  ollama: null  # Ollama doesn't require API key for local usage
