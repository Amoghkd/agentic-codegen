# Example: Local Ollama Configuration
# This configuration uses local Ollama models for agents

# Available LLM Providers
providers:
  google:
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    models:
      - "gemini-1.5-flash-latest"
      - "gemini-1.5-pro-latest"
      - "gemini-pro"
    default_params:
      temperature: 0.1
      
  openai:
    class: "langchain_openai.ChatOpenAI"
    models:
      - "gpt-4o"
      - "gpt-4o-mini"
      - "gpt-3.5-turbo"
    default_params:
      temperature: 0.1
      
  anthropic:
    class: "langchain_anthropic.ChatAnthropic"
    models:
      - "claude-3-5-sonnet-20241022"
      - "claude-3-haiku-20240307"
    default_params:
      temperature: 0.1

  ollama:
    class: "langchain_community.llms.Ollama"
    base_url: "http://localhost:11434"  # Default Ollama URL
    models:
      - "llama3.2:latest"
      - "llama3.1:latest" 
      - "llama3:latest"
      - "codellama:latest"       # Great for code generation
      - "mistral:latest"
      - "phi3:latest"
      - "qwen2.5:latest"
      - "deepseek-coder:latest"  # Excellent for coding tasks
      - "starcoder2:latest"      # Another good coding model
    default_params:
      temperature: 0.1

# Agent-specific configurations - Using local Ollama models
agents:
  planner:
    provider: "ollama"
    model: "llama3.2:latest"  # Use Llama for planning
    temperature: 0.1
    description: "Strategic planning with local Llama model"
    
  code_generator:
    provider: "ollama"
    model: "codellama:latest"  # Use CodeLlama for code generation
    temperature: 0.2
    description: "Code generation with local CodeLlama"
    
  corrector:
    provider: "ollama"
    model: "deepseek-coder:latest"  # Use DeepSeek for corrections
    temperature: 0.1
    description: "Code correction with local DeepSeek Coder"
    
  critique:
    provider: "ollama"
    model: "llama3.2:latest"  # Use Llama for code review
    temperature: 0.1
    description: "Code review with local Llama model"
    
  reasoner:
    provider: "ollama"
    model: "qwen2.5:latest"  # Use Qwen for reasoning
    temperature: 0.1
    description: "Symbolic reasoning with local Qwen model"
    
  verifier:
    provider: "ollama"
    model: "codellama:latest"  # Use CodeLlama for verification
    temperature: 0.0
    description: "Code verification with local CodeLlama"

  docker_agent:
    provider: "ollama"
    model: "llama3.2:latest"  # Use Llama for Docker analysis
    temperature: 0.3
    description: "Docker analysis with local Llama model"

# Global defaults
defaults:
  provider: "ollama"
  model: "llama3.2:latest"
  temperature: 0.1
  max_tokens: 4096
  timeout: 30

# Environment variables mapping
env_vars:
  google: "GOOGLE_API_KEY"
  openai: "OPENAI_API_KEY"
  anthropic: "ANTHROPIC_API_KEY"
  ollama: null  # Ollama doesn't require API key for local usage
